{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIS 9\n",
    "\n",
    "## Supervised Learning: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading\n",
    "- Python Data Handbook Chapter 5: Decision Trees\n",
    "- Python Data Handbook Chapter 5: Gaussian Naive Bayes\n",
    "- Think Stats: Probability - Bayes's Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification, or classification predictive modeling, uses algorithms that learn from a set of input features X and associated labels y, such that when presented with a new set of features X, the algorithms can correctly predict the y outcome. On the surface, classification sounds like regression: using training data X and y to learn, then correctly predict the output y for some new X. And that's because both classification and regression are supervised learning.\n",
    "\n",
    "The difference between regression and classification is in the type of X and y data.\n",
    "- For regression, the X and predicted y data are _quantitative_ data, or a number within a range of values. An example of quantitative data is the cost of a car within a range of 0 to 200,000 dollars.\n",
    "- For classification, the X and predicted y data are _categorical_ data, or a type of object from a set of different types. An example of categorical data is the type of car from the categories of compact, sport, sedan, or luxury cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Decision Tree__\n",
    "\n",
    "A decision tree is one of the most common classification model. The decision tree is a flow chart that is a tree structure. There is a root note at the top, which branches into 2 child nodes, a left child and right child. Each child node, in turn, branches into 2 more child nodes, until the path reaches a terminal or end node called a leaf node.\n",
    "\n",
    "Every node holds a test for an X attribute. The result of the test (True or False) determines which of the 2 branches will be selected. The algorithm starts at the root node and traverses down the tree. At each node it tests the X attribute and then takes the resulting path to reach the next child node. When it reaches the leaf node, the value at the leaf node is the output y label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use data in the file `zoo.csv` (from https://www.kaggle.com/uciml/zoo-animal-classification). The dataset consists of features of different animals at a zoo, and the animals are divided into 7 different types: 4-legged animals, fish, birds, crustaceans, amphibians, insects, reptiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. read data into a DataFrame called __a__. Then print the size and first 5 lines of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create the X data, which is the DataFrame a but with no 'animal_name' or 'class_type' columns.\n",
    "<br>Create the y data, which is the 'class_type' column.\n",
    "<br>Print the size of X and y, and show the first 5 lines of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Note that the data in x are conveniently numbers already. Otherwise we'd have to convert any text strings into numbers.\n",
    "<br>Split the data into a training set and a test set.\n",
    "<br>Show the size of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create the decision tree classifier and train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regr = DecisionTreeRegressor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Inspect the decision tree that was created by the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8)\n",
    "tree.plot_tree(regr, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. See how well the classifier works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)   \n",
    "# accuracy score between 0 and 1, with 1 being 100% accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to see how well the classifier works is with the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred, labels=np.arange(1,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are the predicted values (or y_pred) and the rows are the actual values (or y_test).\n",
    "<br>With a 100% accurate score, the matrix will have non-zeros only across the diagonal from [0,0] to [n,n].\n",
    "<br>If there are values not on the diagonal, it means the classifier made a wrong prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending from the Decision Tree model is the __Random Forest__ model. For some applications one decision tree is not sufficient to produce effective output. Random Forest is an algorithm that uses multiple decision trees (or a forest of trees) to make decisions. The algorithm randomly creates decision trees, and each node in a decision trees has a random subset of features to calculate the output. The algorithm then combines the output of the individual decision trees to get the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__K Nearest Neighbor__\n",
    "\n",
    "The K Nearest Neighbor, or KNN, algorithm is called a lazy learning algorithm, which means it does not use any training data to generate the model. This is in contrast to the Decision Tree algorithm, which generates the tree during training. For KNN, all training data are only stored but not used during the training stage. During the testing stage the algorithm will use the training data with the test data to make the predictions. This makes training faster but makes the testing slower.\n",
    "\n",
    "To predict an outcome, the algorithm plots the new data with the stored training data. Then it locates the training data points that are closest to the new data. These nearest training data points, or the nearest neighbors, determine the output prediction of the new data. For example, if the features of the new data cause it to be plotted near training data of type T, then the prediction for the new data will be type T.\n",
    "\n",
    "In KNN, K is the number of nearest neighbors that the algorithm will use to make the prediction. We can adjust the K value to help the model be more accurate. A small K value may not be as accurate as a larger K value, but a K value that's too large will include neighbors that are too far away and introduce errors into the evaluation or be computationally costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. We use the same data from `zoo.csv` above, so we can still use the same training and testing data.\n",
    "<br>Create a KNN classifer, train and test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Check the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred, labels=np.arange(1,8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive Bayes__\n",
    "\n",
    "The Naive Bayes model is based on the Bayesian theorem, which calculates the probability of an output label given some observed features. For example, if the features are dark clouds and strong winds, then the probability for stormy weather is high and the probability for sunny weather is low.\n",
    "\n",
    "When calculating the probability of the output label, the model assumes that all features are independent of each other, which isn't always true, and that's the reason why it's called naive. \n",
    "\n",
    "We start by investigating the Gaussian Naive Bayes model. The Gaussian term means that we assume the features have a Gaussian distribution, which is a common distribution for many events or measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. We start by observing the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that many features are independent, such as being aquatic and being a predator, but some features are dependent, such as having hair means not having feathers. \n",
    "<br>We also see that uf an animal has hair, milk, is a predator with teeth, backbone, and other features of a bear, then the probability of it being a bear is high, and the model will classify this animal as a bear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Use the same training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Create the classifier, train the classifier, test the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of Naive Bayes classification is the __Multinomial Naive Bayes__ model. The model also uses the combined probabilities of the features to predict the output, but it also takes into account the number of occurrences of a feature. \n",
    "\n",
    "Multinomial Naive Bayes is often used for classification of text or reading material. For example, if a reading passage has the words 'Congress', 'election', 'vote', 'political', '2020', then the probability of it being about the 2020 election is reasonable, but if the '2020' appears 10 times, then the probability of it being about the 2020 election is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've discussed 3 of the common classification models in supervised learning and how they work in general. It takes some more in-depth knowledge and statistical background to _tune_ or adjust a model so that it can be more accurate with a particular application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
